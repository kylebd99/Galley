{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Finch\n",
    "using Finch: @finch_program_instance\n",
    "using SparseArrays\n",
    "using PrettyPrinting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence_instance(declare_instance(variable_instance(:t, t), literal_instance(0)), loop_instance(index_instance(:i), loop_instance(index_instance(:j), assign_instance(access_instance(variable_instance(:t, t), updater_instance(create_instance()), index_instance(:j), index_instance(:i)), literal_instance(Finch.FinchNotation.InitWriter{0.0}()), call_instance(variable_instance(:*, *), access_instance(variable_instance(:A, A), reader_instance(), index_instance(:j), index_instance(:i)), access_instance(variable_instance(:A, A), reader_instance(), index_instance(:j), index_instance(:i)))))))\n",
      "\n",
      "Fiber(SparseList{Int64, Int64}(SparseList{Int64, Int64}(Element{0.0, Float64}([1.0, 4.0, 9.0, 1.0, 4.0, 9.0, 1.0, 4.0, 9.0, 1.0, 4.0, 9.0]), 4, [1, 4, 7, 10, 13], [2, 3, 4, 2, 3, 4, 2, 3, 4, 2, 3, 4]), 4, [1, 5], [1, 2, 3, 4]))\n",
      "12\n",
      "call_instance(variable_instance(:*, *), access_instance(variable_instance(:A, A), reader_instance(), index_instance(:j), index_instance(:i)), access_instance(variable_instance(:A, A), reader_instance(), index_instance(:j), index_instance(:i)))\n",
      "sequence_instance(declare_instance(variable_instance(:t, t), literal_instance(0)), loop_instance(index_instance(:i), loop_instance(index_instance(:j), assign_instance(access_instance(variable_instance(:t, t), updater_instance(create_instance()), index_instance(:j), index_instance(:i)), literal_instance(Finch.FinchNotation.InitWriter{0.0}()), call_instance(variable_instance(:f, f), variable_instance(:a_access, a_access), variable_instance(:a_access, a_access))))))\n",
      "\n",
      "Fiber(SparseList{Int64, Int64}(SparseList{Int64, Int64}(Element{0.0, Float64}([1.0, 4.0, 9.0, 1.0, 4.0, 9.0, 1.0, 4.0, 9.0, 1.0, 4.0, 9.0]), 4, [1, 4, 7, 10, 13], [2, 3, 4, 2, 3, 4, 2, 3, 4, 2, 3, 4]), 4, [1, 5], [1, 2, 3, 4]))\n",
      "12\n",
      "true\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"sequence_instance(declare_instance(variable_instance(:t, t), literal_instance(0)), loop_instance(index_instance(:i), loop_instance(index_instance(:j), assign_instance(access_instance(variable_instance(:t, t), updater_instance(create_instance()), index_instance(:j), index\" ⋯ 49 bytes ⋯ \"ion.InitWriter{0.0}()), call_instance(variable_instance(:*, *), access_instance(variable_instance(:A, A), reader_instance(), index_instance(:j), index_instance(:i)), access_instance(variable_instance(:A, A), reader_instance(), index_instance(:j), index_instance(:i)))))))\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "t = @fiber(sl(sl(e(0.0))))\n",
    "A = dropdefaults(copyto!(@fiber(sl(sl(e(0.0)))), SparseMatrixCSC([0 0 0 0; -1 -1 -1 -1; -2 -2 -2 -2; -3 -3 -3 -3])))\n",
    "B = dropdefaults(copyto!(@fiber(sl(sl(e(0.0)))), SparseMatrixCSC([-2 -2 -2 -2; -1 -1 -1 -1; -2 -2 -2 -2; -3 -3 -3 -3])))\n",
    "default_value = 0\n",
    "@finch (t .= 0; @loop i j t[j,i] = A[j, i]*A[j, i])\n",
    "prgm1 = @finch_program_instance (t .= 0; @loop i j t[j,i] = A[j, i]*A[j, i])\n",
    "println(prgm1)\n",
    "println(t)\n",
    "println(countstored(Finch.execute(prgm1).t))\n",
    "\n",
    "tensor_var = Finch.FinchNotation.variable_instance(Symbol(\"A\"), A)\n",
    "index_expressions = [Finch.FinchNotation.index_instance(Symbol(\"j\")), Finch.FinchNotation.index_instance(Symbol(\"i\"))] \n",
    "a_access = @finch_program_instance $(tensor_var)[index_expressions...]\n",
    "child_prgms =  [a_access for _ in 1:2]\n",
    "op = Finch.FinchNotation.call_instance(Finch.FinchNotation.variable_instance(:*, *), a_access, a_access)\n",
    "println(op)\n",
    "f = *\n",
    "prgm = @finch_program_instance (t .= 0; @loop i j t[j,i] = f(a_access, a_access))\n",
    "println(prgm)\n",
    "println(Finch.execute(prgm).t)\n",
    "println(countstored(Finch.execute(prgm).t))\n",
    "println(t == Finch.execute(prgm).t)\n",
    "\n",
    "\n",
    "prgm1_str = \"sequence_instance(declare_instance(variable_instance(:t, t), literal_instance(0)), loop_instance(index_instance(:i), loop_instance(index_instance(:j), assign_instance(access_instance(variable_instance(:t, t), updater_instance(create_instance()), index_instance(:j), index_instance(:i)), literal_instance(Finch.FinchNotation.InitWriter{0.0}()), call_instance(variable_instance(:*, *), access_instance(variable_instance(:A, A), reader_instance(), index_instance(:j), index_instance(:i)), access_instance(variable_instance(:A, A), reader_instance(), index_instance(:j), index_instance(:i)))))))\"\n",
    "prgm2_str = \"sequence_instance(declare_instance(variable_instance(:t, t), literal_instance(0)), loop_instance(index_instance(:i), loop_instance(index_instance(:j), assign_instance(access_instance(variable_instance(:t, t), updater_instance(create_instance()), index_instance(:j), index_instance(:i)), literal_instance(Finch.FinchNotation.InitWriter{0.0}()), call_instance(variable_instance(:*, *), access_instance(variable_instance(:A, A), reader_instance(), index_instance(:j), index_instance(:i)), access_instance(variable_instance(:A, A), reader_instance(), index_instance(:j), index_instance(:i)))))))\"\n",
    "# Possible front-ends for the system\n",
    "#@tensor_save A\n",
    "\n",
    "#query = @init_query\n",
    "#query = @add_line C[i,j] = B[i] +A[j]\n",
    "#query = @add_line C[i,j] = B[i] +A[j]\n",
    "#query = @add_line C[i,j] = B[i] +A[j]\n",
    "#query = @add_line C[i,j] = B[i] +A[j]\n",
    "#result = @execute_query query\n",
    "\n",
    "#@query C[i,j] = B[i] +A[j]\n",
    "#@query D[i,j] = D[i] +A[j]\n",
    "#@print D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2×2×3 Array{Int64, 3}:\n",
       "[:, :, 1] =\n",
       " 1  3\n",
       " 2  4\n",
       "\n",
       "[:, :, 2] =\n",
       " 5  7\n",
       " 6  8\n",
       "\n",
       "[:, :, 3] =\n",
       "  9  11\n",
       " 10  12"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = [2, 3]\n",
    "B = [1;2;;3;4;;;5;6;;7;8;;;9;10;;11;12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Vector{Int64}:\n",
       " 2\n",
       " 3"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B[1, 1, 2] + A[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(A .+ B)[1,1,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract type  Number end\n",
    "\n",
    "struct integer <: Number\n",
    "    c::Int\n",
    "    value::Int\n",
    "end\n",
    "\n",
    "struct decimal <: Number\n",
    "    c::Int\n",
    "    value::Float64\n",
    "end\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 692,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "multiplication1 (generic function with 3 methods)"
      ]
     },
     "execution_count": 692,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function multiplication1(output_tensor, input_tensors)\n",
    "    A = input_tensors[1]\n",
    "    B = input_tensors[2]\n",
    "    @finch(@loop(i, j, output_tensor[i, j] = A[i, j] * B[i, j]))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 693,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(output_tensor = Fiber(SparseList{Int64, Int64}(4, [1, 4], [2, 3, 4], SparseList{Int64, Int64}(4, [1, 5, 9, 13], [1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4], Element{0.0, Float64}([1.0, 1.0, 1.0, 1.0, 4.0, 4.0, 4.0, 4.0, 9.0, 9.0, 9.0, 9.0])))),)"
      ]
     },
     "execution_count": 693,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiplication1(t, [A,B])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 702,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "487.7295372841917"
      ]
     },
     "execution_count": 702,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "10000*(1-(9999/10000)^500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5-element Vector{Any}:\n",
       " Symbol(\"@loop\")\n",
       " :(\u001b[90m#= none:1 =#\u001b[39m)\n",
       " :i\n",
       " :j\n",
       " :(t[i, j] = A[i, j] * B[i, j])"
      ]
     },
     "execution_count": 581,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Meta.parse(\"@finch @loop i j t[i,j] = A[i,j] * B[i,j]\").args[3].args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "multiplication2 (generic function with 1 method)"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function multiplication2()\n",
    "    t1 = @fiber(sl(sl(e(0.0))))\n",
    "    A1 = dropdefaults(copyto!(@fiber(sl(sl(e(0.0)))), SparseMatrixCSC([0 0 0 0; -1 -1 -1 -1; -2 -2 -2 -2; -3 -3 -3 -3])))\n",
    "    B1 = dropdefaults(copyto!(@fiber(sl(sl(e(0.0)))), SparseMatrixCSC([0 0 0 0; -1 -1 -1 -1; -2 -2 -2 -2; -3 -3 -3 -3])))\n",
    "    Meta.parse(\"@finch @loop i j t1[i,j] = A1[i,j] * B1[i,j]\")\n",
    "    return t1\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseList (0.0) [1:0]\n",
       "│ \n"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiplication2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "multiplication (generic function with 2 methods)"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function multiplication(indices, input_fibers, output_fiber)\n",
    "    println(input_symbols)\n",
    "    multiplication_expr = :(:call, )\n",
    "    query = \"@finch @loop \"\n",
    "    for index in indices\n",
    "        query *= index * \" \"\n",
    "    end\n",
    "    var\"t\" = output_fiber\n",
    "    query *= \"t[\"\n",
    "    comma_prefix = \"\"\n",
    "    for index in indices\n",
    "        query *= comma_prefix * index\n",
    "        comma_prefix = \",\" \n",
    "    end\n",
    "    query *= \"] = \"\n",
    "    prefix = \"\"\n",
    "    for i in range(1, length(input_fibers))\n",
    "        query *= prefix * input_symbols[i] * \"[\"\n",
    "        comma_prefix = \"\"\n",
    "        for index in indices\n",
    "            query *= comma_prefix * index\n",
    "            comma_prefix = \",\" \n",
    "        end\n",
    "        query *= \"]\"\n",
    "        prefix = \" * \"\n",
    "    end\n",
    "    return eval(Meta.parse(query))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Symbol(\"Fiber(SparseList{Int64, Int64}(4, [1, 4], [2, 3, 4], SparseList{Int64, Int64}(4, [1, 5, 9, 13], [1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4], Element{0.0, Float64}([-1.0, -1.0, -1.0, -1.0, -2.0, -2.0, -2.0, -2.0, -3.0, -3.0, -3.0, -3.0]))))\"), Symbol(\"Fiber(SparseList{Int64, Int64}(4, [1, 4], [2, 3, 4], SparseList{Int64, Int64}(4, [1, 5, 9, 13], [1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4], Element{0.0, Float64}([-1.0, -1.0, -1.0, -1.0, -2.0, -2.0, -2.0, -2.0, -3.0, -3.0, -3.0, -3.0]))))\"), Symbol(\"Fiber(SparseList{Int64, Int64}(4, [1, 4], [2, 3, 4], SparseList{Int64, Int64}(4, [1, 5, 9, 13], [1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4], Element{0.0, Float64}([-1.0, -1.0, -1.0, -1.0, -2.0, -2.0, -2.0, -2.0, -3.0, -3.0, -3.0, -3.0]))))\"), Symbol(\"Fiber(SparseList{Int64, Int64}(4, [1, 4], [2, 3, 4], SparseList{Int64, Int64}(4, [1, 5, 9, 13], [1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4], Element{0.0, Float64}([-1.0, -1.0, -1.0, -1.0, -2.0, -2.0, -2.0, -2.0, -3.0, -3.0, -3.0, -3.0]))))\")]\n"
     ]
    },
    {
     "ename": "LoadError",
     "evalue": "MethodError: no method matching *(::String, ::Symbol)\n\u001b[0mClosest candidates are:\n\u001b[0m  *(::Any, ::Any, \u001b[91m::Any\u001b[39m, \u001b[91m::Any...\u001b[39m) at operators.jl:591\n\u001b[0m  *(::Union{AbstractChar, AbstractString}, \u001b[91m::Union{AbstractChar, AbstractString}...\u001b[39m) at strings/basic.jl:260\n\u001b[0m  *(::Union{AbstractChar, AbstractString}, \u001b[91m::Missing\u001b[39m) at missing.jl:183\n\u001b[0m  ...",
     "output_type": "error",
     "traceback": [
      "MethodError: no method matching *(::String, ::Symbol)\n\u001b[0mClosest candidates are:\n\u001b[0m  *(::Any, ::Any, \u001b[91m::Any\u001b[39m, \u001b[91m::Any...\u001b[39m) at operators.jl:591\n\u001b[0m  *(::Union{AbstractChar, AbstractString}, \u001b[91m::Union{AbstractChar, AbstractString}...\u001b[39m) at strings/basic.jl:260\n\u001b[0m  *(::Union{AbstractChar, AbstractString}, \u001b[91m::Missing\u001b[39m) at missing.jl:183\n\u001b[0m  ...",
      "",
      "Stacktrace:",
      " [1] *",
      "   @ ./operators.jl:591 [inlined]",
      " [2] multiplication(indices::Vector{String}, input_fibers::Vector{Fiber{Finch.SparseListLevel{Int64, Int64, Finch.SparseListLevel{Int64, Int64, Finch.ElementLevel{0.0, Float64}}}, Finch.Environment{NamedTuple{(), Tuple{}}}}}, output_fiber::Fiber{Finch.SparseListLevel{Int64, Int64, Finch.SparseListLevel{Int64, Int64, Finch.ElementLevel{0.0, Float64}}}, Finch.Environment{NamedTuple{(), Tuple{}}}})",
      "   @ Main ./In[307]:19",
      " [3] top-level scope",
      "   @ In[308]:1",
      " [4] eval",
      "   @ ./boot.jl:368 [inlined]",
      " [5] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "   @ Base ./loading.jl:1428"
     ]
    }
   ],
   "source": [
    "multiplication([\"i\", \"j\"], [A, B, A, A], t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(C = Fiber(Element{0.0, Float64}([6.0])),)"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C = @fiber(e(0.0))\n",
    "B = copyto!(@fiber(sl(e(0.0))), [1, 1, 1, 1])\n",
    "A = copyto!(@fiber(sl(sl(e(0.0)))), SparseMatrixCSC([1 1 1 1; 1 1 1 1; 1 1 1 1; 1 1 1 1]))\n",
    "@finch @loop i j C[] <<max>>= A[i, j] * B[i] + 5*B[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(C = Fiber(SparseList{Int64, Int64}(4, [1, 5], [1, 2, 3, 4], Element{0.0, Float64}([4.0, 8.0, 12.0, 16.0]))),)"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C = @fiber(sl(e(0.0)))\n",
    "A = copyto!(@fiber(sl(sl(e(0.0)))), SparseMatrixCSC([1 2 3 4; 5 6 7 8; 9 10 11 12; 13 14 15 16]))\n",
    "@finch @loop i j C[i] <<max>>= A[i, j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "einreduce_with_protocols (generic function with 1 method)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Finch\n",
    "using Finch: @finch_program_instance\n",
    "using SparseArrays\n",
    "\n",
    "function pointwise_multiplication(indices, As)\n",
    "    B = @fiber(sl(sl((e(0)))))\n",
    "    isempty(As) && return B\n",
    "    indices = [Finch.FinchNotation.index_instance(Symbol(i)) for i in indices]\n",
    "    A_vars = [Finch.FinchNotation.variable_instance(Symbol(:A, n), As[n]) for n in 1:length(As)]\n",
    "    #create a list of variable instances with different names to hold the input tensors\n",
    "    ex = @finch_program_instance 1\n",
    "    for A_var in A_vars\n",
    "        ex = @finch_program_instance $A_var[indices...] * $ex\n",
    "    end \n",
    "    prgm = @finch_program_instance @loop reverse(indices)... B[indices...] = $ex\n",
    "    return Finch.execute(prgm).B\n",
    "end\n",
    "\n",
    "function pointwise_binary_op(indices, As, op)\n",
    "    B = @fiber(sl(sl((e(0)))))\n",
    "    isempty(As) && return B\n",
    "    indices = [Finch.FinchNotation.index_instance(Symbol(i)) for i in indices]\n",
    "    A_vars = [Finch.FinchNotation.variable_instance(Symbol(:A, n), As[n]) for n in 1:length(As)]\n",
    "    #create a list of variable instances with different names to hold the input tensors\n",
    "    ex = @finch_program_instance 1\n",
    "    for A_var in A_vars\n",
    "        ex = @finch_program_instance op($A_var[indices...], $ex)\n",
    "    end \n",
    "    prgm = @finch_program_instance @loop reverse(indices)... B[indices...] = $ex\n",
    "    return Finch.execute(prgm).B\n",
    "end\n",
    "\n",
    "function binary_op(all_indices, As_indices, As, op)\n",
    "    B = @fiber(sl(sl((e(0)))))\n",
    "    isempty(As) && return B\n",
    "    all_indices = [Finch.FinchNotation.index_instance(Symbol(i)) for i in all_indices]\n",
    "    As_indices = [[Finch.FinchNotation.index_instance(Symbol(i)) for i in indices] for indices in As_indices]\n",
    "    A_vars = [Finch.FinchNotation.variable_instance(Symbol(:A, n), As[n]) for n in 1:length(As)]\n",
    "    #create a list of variable instances with different names to hold the input tensors\n",
    "    ex = @finch_program_instance 1\n",
    "    for i in range(1, length(As))\n",
    "        A_var = A_vars[i]\n",
    "        indices = As_indices[i]\n",
    "        ex = @finch_program_instance op($A_var[indices...], $ex)\n",
    "    end \n",
    "    prgm = @finch_program_instance @loop reverse(all_indices)... B[all_indices...] = $ex\n",
    "    return Finch.execute(prgm).B\n",
    "end\n",
    "\n",
    "function binary_ops(all_indices, As_indices, As, ops)\n",
    "    B = @fiber(sl(sl((e(0)))))\n",
    "    isempty(As) && return B\n",
    "    all_indices = [Finch.FinchNotation.index_instance(Symbol(i)) for i in all_indices]\n",
    "    As_indices = [[Finch.FinchNotation.index_instance(Symbol(i)) for i in indices] for indices in As_indices]\n",
    "    A_vars = [Finch.FinchNotation.variable_instance(Symbol(:A, n), As[n]) for n in 1:length(As)]\n",
    "    #create a list of variable instances with different names to hold the input tensors\n",
    "    ex = @finch_program_instance $(A_vars[1])[As_indices[1]...]\n",
    "    for i in range(2, length(As))\n",
    "        A_var = A_vars[i]\n",
    "        indices = As_indices[i]\n",
    "        op = ops[i-1]\n",
    "        ex = @finch_program_instance op($A_var[indices...], $ex)\n",
    "    end \n",
    "    prgm = @finch_program_instance @loop reverse(all_indices)... B[all_indices...] = $ex\n",
    "    return Finch.execute(prgm).B\n",
    "end\n",
    "\n",
    "using Finch: SparseListLevel, Element, fiber\n",
    "\n",
    "function einsum(all_indices, output_indices, As_indices, As, ops)\n",
    "    B = Element(0.0)\n",
    "    for _ in output_indices\n",
    "        B = SparseListLevel(B)\n",
    "    end\n",
    "    B = Fiber!(B)\n",
    "    isempty(As) && return B\n",
    "    all_indices = [Finch.FinchNotation.index_instance(Symbol(i)) for i in all_indices]\n",
    "    output_indices = [Finch.FinchNotation.index_instance(Symbol(i)) for i in output_indices]\n",
    "    As_indices = [[Finch.FinchNotation.index_instance(Symbol(i)) for i in indices] for indices in As_indices]\n",
    "    A_vars = [Finch.FinchNotation.variable_instance(Symbol(:A, n), As[n]) for n in 1:length(As)]\n",
    "    #create a list of variable instances with different names to hold the input tensors\n",
    "    ex = @finch_program_instance $(A_vars[1])[As_indices[1]...]\n",
    "    for i in range(2, length(As))\n",
    "        A_var = A_vars[i]\n",
    "        indices = As_indices[i]\n",
    "        op = ops[i-1]\n",
    "        ex = @finch_program_instance op($A_var[indices...], $ex)\n",
    "    end \n",
    "    prgm = @finch_program_instance @loop reverse(all_indices)... B[output_indices...] <<min>>= $ex\n",
    "    return Finch.execute(prgm).B\n",
    "end\n",
    "\n",
    "function einreduce(all_indices, output_indices, As_indices, As, ops, reduce_op)\n",
    "    B = Element(0.0)\n",
    "    for _ in output_indices\n",
    "        B = SparseListLevel(B)\n",
    "    end\n",
    "    B = Fiber!(B)\n",
    "    isempty(As) && return B\n",
    "    all_indices = [Finch.FinchNotation.index_instance(Symbol(i)) for i in all_indices]\n",
    "    output_indices = [Finch.FinchNotation.index_instance(Symbol(i)) for i in output_indices]\n",
    "    As_indices = [[Finch.FinchNotation.index_instance(Symbol(i)) for i in indices] for indices in As_indices]\n",
    "    A_vars = [Finch.FinchNotation.variable_instance(Symbol(:A, n), As[n]) for n in 1:length(As)]\n",
    "    #create a list of variable instances with different names to hold the input tensors\n",
    "    ex = @finch_program_instance $(A_vars[1])[As_indices[1]...]\n",
    "    for i in range(2, length(As))\n",
    "        A_var = A_vars[i]\n",
    "        indices = As_indices[i]\n",
    "        op = ops[i-1]\n",
    "        ex = @finch_program_instance op($A_var[indices...], $ex)\n",
    "    end \n",
    "    prgm = @finch_program_instance @loop reverse(all_indices)... B[output_indices...] <<reduce_op>>= $ex\n",
    "    return Finch.execute(prgm).B\n",
    "end\n",
    "\n",
    "function einreduce_with_protocols(all_indices, output_indices, As_indices_and_protocols, As, ops, reduce_op)\n",
    "    B = Element(0.0)\n",
    "    for _ in output_indices\n",
    "        B = SparseListLevel(B)\n",
    "    end\n",
    "    B = Fiber!(B)\n",
    "    isempty(As) && return B\n",
    "    all_indices = [Finch.FinchNotation.index_instance(Symbol(i)) for i in all_indices]\n",
    "    output_indices = [Finch.FinchNotation.index_instance(Symbol(i)) for i in output_indices]\n",
    "    As_indices = [[(Finch.FinchNotation.index_instance(Symbol(i[1])), i[2]) for i in indices] for indices in As_indices_and_protocols]\n",
    "    As_protocols = []\n",
    "    for indices in As_indices\n",
    "        protocols = []\n",
    "        for i in range(1, length(indices))\n",
    "            index = indices[i][1]\n",
    "            protocol = indices[i][2]\n",
    "            push!(protocols, @finch_program_instance index::protocol)\n",
    "        end\n",
    "        push!(As_protocols, protocols)\n",
    "    end\n",
    "    A_vars = [Finch.FinchNotation.variable_instance(Symbol(:A, n), As[n]) for n in 1:length(As)]\n",
    "    #create a list of variable instances with different names to hold the input tensors\n",
    "    ex = @finch_program_instance $(A_vars[1])[As_protocols[1]...]\n",
    "    for i in range(2, length(As))\n",
    "        A_var = A_vars[1]\n",
    "        op_As =  [@finch_program_instance $A_var[As_protocols[i]...]]\n",
    "        op = ops[i-1]\n",
    "        ex = @finch_program_instance op(op_As..., $ex)\n",
    "    end \n",
    "    prgm = @finch_program_instance @loop reverse(all_indices)... B[output_indices...] <<reduce_op>>= $ex\n",
    "    return Finch.execute(prgm).B\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseList (0.0) [:,1:10]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = @fiber(sl(sl(e(0.0), 10), 10))\n",
    "#A = copyto!(@fiber(sl(sl(e(0.0)))), SparseMatrixCSC([1 2 3 4; 5 6 7 8; 9 10 11 12; 13 14 15 16]))\n",
    "#indices = [\"i\", \"j\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fiber(SparseList{Int64, Int64}(SparseList{Int64, Int64}(Element{0, Int64}([1, 125, 729, 2197, 8, 216, 1000, 2744, 27, 343, 1331, 3375, 64, 512, 1728, 4096]), 4, [1, 5, 9, 13, 17], [1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4]), 4, [1, 5], [1, 2, 3, 4]))\n",
      "Fiber(SparseList{Int64, Int64}(SparseList{Int64, Int64}(Element{0, Int64}([1, 125, 729, 2197, 8, 216, 1000, 2744, 27, 343, 1331, 3375, 64, 512, 1728, 4096]), 4, [1, 5, 9, 13, 17], [1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4]), 4, [1, 5], [1, 2, 3, 4]))\n",
      "Fiber(SparseList{Int64, Int64}(SparseList{Int64, Int64}(Element{0, Int64}([1, 125, 729, 2197, 8, 216, 1000, 2744, 27, 343, 1331, 3375, 64, 512, 1728, 4096]), 4, [1, 5, 9, 13, 17], [1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4]), 4, [1, 5], [1, 2, 3, 4]))\n",
      "Fiber(SparseList{Int64, Int64}(SparseList{Int64, Int64}(Element{0, Int64}([1, 5, 9, 13, 2, 6, 10, 14, 3, 7, 11, 15, 4, 8, 12, 16]), 4, [1, 5, 9, 13, 17], [1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4]), 4, [1, 5], [1, 2, 3, 4]))\n",
      "Fiber(SparseList{Int64, Int64}(Element{0.0, Float64}([0.0, 0.0, 0.0, 0.0]), 4, [1, 5], [1, 2, 3, 4]))\n",
      "Fiber(Element{0.0, Float64}([0.0]))\n",
      "Fiber(SparseList{Int64, Int64}(SparseList{Int64, Int64}(Element{0.0, Float64}([1.0, 5.0, 9.0, 13.0, 2.0, 6.0, 10.0, 14.0, 3.0, 7.0, 11.0, 15.0, 4.0, 8.0, 12.0, 16.0]), 4, [1, 5, 9, 13, 17], [1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4]), 4, [1, 5], [1, 2, 3, 4]))\n",
      "Fiber(Element{0.0, Float64}([272.0]))\n"
     ]
    }
   ],
   "source": [
    "println(pointwise_multiplication(indices, [A, A, A]))\n",
    "println(pointwise_binary_op(indices, [A, A, A], *))\n",
    "println(binary_op(indices, [indices for _ in range(1, 3)], [A, A, A], *))\n",
    "println(binary_ops(indices, [indices for _ in range(1, 3)], [A, A, A], [+, min]))\n",
    "println(einsum(indices, [\"j\"], [indices for _ in range(1, 3)], [A, A, A], [+, min]))\n",
    "println(einsum(indices, [], [indices for _ in range(1, 3)], [A, A, A], [+, min]))\n",
    "println(einreduce(indices, [\"i\", \"j\"], [indices for _ in range(1, 3)], [A, A, A], [+, min], (x,y) -> max(x,y) ))\n",
    "println(einreduce_with_protocols(indices, [], [[(x, walk) for x in indices] for _ in range(1, 2)], [A, A], [+], +))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "execute_tensor_kernel (generic function with 1 method)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Finch\n",
    "using Finch: @finch_program_instance\n",
    "using DataStructures\n",
    "include(\"../Source/TensorQuery.jl\")\n",
    "\n",
    "# The set of allowed level formats provided by the Finch API\n",
    "@enum LevelFormat t_sparse_list = 1 t_dense = 2\n",
    "\n",
    "# The struct containing all information needed to compute a small tensor kernel using the finch compiler.\n",
    "# This will be the output of the kernel optimizer\n",
    "struct TensorKernel \n",
    "    kernel_root::TensorExpression\n",
    "\n",
    "    input_tensors::Dict{TensorId, Finch.Fiber} \n",
    "    input_indices::Dict{TensorId, Vector{String}}\n",
    "    input_protocols::Dict{TensorId, Vector{AccessProtocol}}\n",
    "    \n",
    "    output_indices::Vector{String}\n",
    "    output_formats::Vector{LevelFormat}\n",
    "    \n",
    "    loop_order::Vector{String}\n",
    "end\n",
    "\n",
    "function initialize_tensor(formats::Vector{LevelFormat}, dims::Vector{Int64}, default_value)\n",
    "    B = Element(default_value)\n",
    "    for i in range(1, length(dims))\n",
    "        if formats[i] == t_sparse_list\n",
    "            B = SparseList(B, dims[i])\n",
    "            println(B)\n",
    "        elseif formats[i] == t_dense\n",
    "            B = Dense(B, dims[i])\n",
    "        else\n",
    "            println(\"Error: Attempted to initialize invalid level format type.\")\n",
    "        end\n",
    "    end\n",
    "    return Fiber!(B)\n",
    "end\n",
    "\n",
    "function determine_default_value(kernel::TensorKernel)\n",
    "    nodes_to_visit = Queue{Tuple{TensorExpression, Int64}}()\n",
    "    node_dict = Dict()\n",
    "    node_id_counter = 0\n",
    "    enqueue!(nodes_to_visit, (kernel.kernel_root, node_id_counter))\n",
    "    while length(nodes_to_visit) > 0\n",
    "        cur_node, cur_node_id = dequeue!(nodes_to_visit)\n",
    "        child_node_ids = []\n",
    "        if typeof(cur_node) == OperatorExp\n",
    "            for child_node in cur_node.inputs\n",
    "                node_id_counter += 1\n",
    "                enqueue!(nodes_to_visit, (child_node, node_id_counter))\n",
    "                push!(child_node_ids, node_id_counter)\n",
    "            end\n",
    "        elseif typeof(cur_node) == AggregateExp\n",
    "            node_id_counter += 1\n",
    "            enqueue!(nodes_to_visit, (cur_node.input, node_id_counter))\n",
    "            push!(child_node_ids, node_id_counter)\n",
    "        end\n",
    "        node_dict[cur_node_id] = (cur_node, child_node_ids)\n",
    "    end\n",
    "\n",
    "    default_value = nothing\n",
    "    for node_id in reverse(range(0, length(keys(node_dict))-1))\n",
    "        node, child_node_ids = node_dict[node_id]\n",
    "        if typeof(node) == TensorAccess\n",
    "            node_dict[node_id] = Finch.default(kernel.input_tensors[node.tensor_id])\n",
    "        elseif typeof(node) == OperatorExp\n",
    "            child_vals = [node_dict[x] for x in child_node_ids]\n",
    "            node_dict[node_id] = node.op(child_vals...)\n",
    "        end\n",
    "        if node_id == 0\n",
    "            if typeof(node) == AggregateExp\n",
    "                default_value = node_dict[child_node_ids[1]]\n",
    "            else\n",
    "                default_value = node_dict[node_id]\n",
    "            end\n",
    "        elseif typeof(node) == AggregateExp\n",
    "            throw(ArgumentError(\"Cannot have an aggregate in the middle of a tensor kernel. They must always occur as the outermost operator.\"))\n",
    "        end\n",
    "    end\n",
    "    return default_value\n",
    "end\n",
    "\n",
    "function initialize_access(tensor_id::TensorId, tensor::Fiber, index_ids::Vector{String}, protocols::Vector{AccessProtocol})\n",
    "    index_expressions = []\n",
    "    for i in range(1, length(index_ids))\n",
    "        index = Finch.FinchNotation.index_instance(Symbol(index_ids[i]))\n",
    "        protocol = nothing\n",
    "        if protocols[i] == t_walk\n",
    "            protocol = walk\n",
    "        elseif protocols[i] == t_fast_walk\n",
    "            protocol = fastwalk\n",
    "        elseif protocols[i] == t_follow\n",
    "            protocol = follow\n",
    "        elseif protocols[i] == t_lead\n",
    "            protocol = lead\n",
    "        elseif protocols[i] == t_gallop\n",
    "            protocol = gallop\n",
    "        end\n",
    "        push!(index_expressions, @finch_program_instance index::protocol)\n",
    "    end\n",
    "    tensor_var = Finch.FinchNotation.variable_instance(Symbol(tensor_id), tensor)\n",
    "    return @finch_program_instance $(tensor_var)[index_expressions...]\n",
    "end\n",
    "\n",
    "function execute_tensor_kernel(kernel::TensorKernel)\n",
    "    loop_order = [Finch.FinchNotation.index_instance(Symbol(i)) for i in kernel.loop_order]\n",
    "    output_indices = [Finch.FinchNotation.index_instance(Symbol(i)) for i in kernel.output_indices]\n",
    "    output_dimensions = Vector{Int64}()\n",
    "    for index in kernel.output_indices\n",
    "        for tensor_id in keys(kernel.input_indices)\n",
    "            input_dim_number = findfirst(==(index), kernel.input_indices[tensor_id])\n",
    "            if isa(input_dim_number, Int64)\n",
    "                push!(output_dimensions, size(kernel.input_tensors[tensor_id])[input_dim_number])\n",
    "                break\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    output_tensor = initialize_tensor(kernel.output_formats, output_dimensions, determine_default_value(kernel))\n",
    "\n",
    "    tensor_accesses = Dict()\n",
    "    for tensor_id in keys(kernel.input_tensors)\n",
    "        tensor_accesses[tensor_id] = initialize_access(tensor_id, kernel.input_tensors[tensor_id], kernel.input_indices[tensor_id], kernel.input_protocols[tensor_id])\n",
    "    end\n",
    "\n",
    "    nodes_to_visit = Queue{Tuple{TensorExpression, Int64}}()\n",
    "    node_dict = Dict()\n",
    "    node_id_counter = 0\n",
    "    enqueue!(nodes_to_visit, (kernel.kernel_root, node_id_counter))\n",
    "    while length(nodes_to_visit) > 0\n",
    "        cur_node, cur_node_id = dequeue!(nodes_to_visit)\n",
    "        child_node_ids = []\n",
    "        if typeof(cur_node) == OperatorExp\n",
    "            for child_node in cur_node.inputs\n",
    "                node_id_counter += 1\n",
    "                enqueue!(nodes_to_visit, (child_node, node_id_counter))\n",
    "                push!(child_node_ids, node_id_counter)\n",
    "            end\n",
    "        elseif typeof(cur_node) == AggregateExp\n",
    "            node_id_counter += 1\n",
    "            enqueue!(nodes_to_visit, (cur_node.input, node_id_counter))\n",
    "            push!(child_node_ids, node_id_counter)\n",
    "        end\n",
    "        node_dict[cur_node_id] = (cur_node, child_node_ids)\n",
    "    end\n",
    "\n",
    "    agg_op = nothing\n",
    "    kernel_prgm = nothing\n",
    "    for node_id in reverse(range(0, length(keys(node_dict))-1))\n",
    "        node, child_node_ids = node_dict[node_id]\n",
    "        if typeof(node) == TensorAccess\n",
    "            node_dict[node_id] = tensor_accesses[node.tensor_id]\n",
    "        elseif typeof(node) == OperatorExp\n",
    "            child_prgms = [node_dict[x] for x in child_node_ids]\n",
    "            node_dict[node_id] = @finch_program_instance $(node.op)(child_prgms...)\n",
    "        end\n",
    "        if node_id == 0\n",
    "            if typeof(node) == AggregateExp\n",
    "                kernel_prgm = node_dict[child_node_ids[1]]\n",
    "                agg_op = node.op\n",
    "            else\n",
    "                kernel_prgm = node_dict[node_id]\n",
    "            end\n",
    "        elseif typeof(node) == AggregateExp\n",
    "            throw(ArgumentError(\"Cannot have an aggregate in the middle of a tensor kernel. They must always occur as the outermost operator.\"))\n",
    "        end\n",
    "    end\n",
    "    if agg_op == nothing\n",
    "        full_prgm = @finch_program_instance @loop loop_order... output_tensor[output_indices...] = $kernel_prgm\n",
    "    else\n",
    "        full_prgm = @finch_program_instance @loop loop_order... output_tensor[output_indices...] <<agg_op>>=  $kernel_prgm\n",
    "    end\n",
    "    return Finch.execute(full_prgm).output_tensor\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparseList{Int64, Int64}(Element{0.0, Float64}([]), 4, [1], [])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SparseList (0.0) [1:4]\n",
       "├─[1]: 40.0\n",
       "├─[2]: 200.0\n",
       "├─[3]: 488.0\n",
       "├─[4]: 904.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_fiber = copyto!(@fiber(d(sl(e(0.0), 4), 4)), [1 2 3 4; 5 6 7 8; 9 10 11 12; 13 14 15 16])\n",
    "A = TensorAccess(\"A\", [\"i\", \"j\"], [t_walk, t_follow])\n",
    "B = TensorAccess(\"B\", [\"i\", \"j\"], [t_walk, t_follow])\n",
    "B_fiber = copyto!(@fiber(d(sl(e(0.0), 4), 4)), [1 2 3 4; 5 6 7 8; 9 10 11 12; 13 14 15 16])\n",
    "C = Mult([A, B])\n",
    "f = +\n",
    "D = CustomOperator(f, [C, B]) # Rename CustomOperator to Map\n",
    "E = CustomAggregate(f, D, [\"i\"]) # Rename CustomAggregate to Fold\n",
    "\n",
    "\n",
    "\n",
    "kernel = TensorKernel(E, Dict(\"A\"=> A_fiber, \"B\"=> B_fiber), Dict(\"A\"=>[\"i\",\"j\"],\"B\"=>[\"i\",\"j\"]), Dict(\"A\"=>[t_walk, t_follow], \"B\"=>[t_walk, t_follow]), [\"i\"], [t_sparse_list], [\"i\", \"j\"])\n",
    "execute_tensor_kernel(kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(T = Fiber(Dense{Int64}(Element{10, Int64}([1, 1, 1, 1]), 4)),)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_fiber = @fiber(d(e(0.0)), [1 ; 1; 1; 1])\n",
    "T = @fiber(d(e(10), 4))\n",
    "@finch @loop i T[i] = A_fiber[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dense [:,1:4]\n",
       "├─[:,1]: SparseList (0.0) [1:4]\n",
       "│ ├─[1]: 1.0\n",
       "│ ├─[2]: 5.0\n",
       "│ ├─[3]: 9.0\n",
       "│ ├─[4]: 13.0\n",
       "├─[:,2]: SparseList (0.0) [1:4]\n",
       "│ ├─[1]: 2.0\n",
       "│ ├─[2]: 6.0\n",
       "│ ├─[3]: 10.0\n",
       "│ ├─[4]: 14.0\n",
       "├─[:,3]: SparseList (0.0) [1:4]\n",
       "│ ├─[1]: 3.0\n",
       "│ ├─[2]: 7.0\n",
       "│ ├─[3]: 11.0\n",
       "│ ├─[4]: 15.0\n",
       "├─[:,4]: SparseList (0.0) [1:4]\n",
       "│ ├─[1]: 4.0\n",
       "│ ├─[2]: 8.0\n",
       "│ ├─[3]: 12.0\n",
       "│ ├─[4]: 16.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B_fiber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseList (0.0) [:,1:4]\n",
       "├─[:,1]: SparseList (0.0) [1:4]\n",
       "│ ├─[1]: 1.0\n",
       "│ ├─[2]: 5.0\n",
       "│ ├─[3]: 9.0\n",
       "│ ├─[4]: 13.0\n",
       "├─[:,2]: SparseList (0.0) [1:4]\n",
       "│ ├─[1]: 2.0\n",
       "│ ├─[2]: 6.0\n",
       "│ ├─[3]: 10.0\n",
       "│ ├─[4]: 14.0\n",
       "├─[:,3]: SparseList (0.0) [1:4]\n",
       "│ ├─[1]: 3.0\n",
       "│ ├─[2]: 7.0\n",
       "│ ├─[3]: 11.0\n",
       "│ ├─[4]: 15.0\n",
       "├─[:,4]: SparseList (0.0) [1:4]\n",
       "│ ├─[1]: 4.0\n",
       "│ ├─[2]: 8.0\n",
       "│ ├─[3]: 12.0\n",
       "│ ├─[4]: 16.0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_fiber\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "access_instance(variable_instance(:A, A), reader_instance(), (index_instance(:i), index_instance(:j)))"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = Finch.FinchNotation.index_instance(:i)\n",
    "j = Finch.FinchNotation.index_instance(:j)\n",
    "indices = [i,j]\n",
    "indices_backwards = [j,i]\n",
    "ex  = @finch_program_instance A[indices...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "access_instance(variable_instance(:A, A), reader_instance(), (index_instance(:i), index_instance(:j)))"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex2  = @finch_program_instance A\n",
    "ex2  = @finch_program_instance $(ex2)[i,j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(t = Fiber(SparseList{Int64, Int64}(SparseList{Int64, Int64}(Element{0.0, Float64}([1.0, 5.0, 9.0, 13.0, 2.0, 6.0, 10.0, 14.0, 3.0, 7.0, 11.0, 15.0, 4.0, 8.0, 12.0, 16.0]), 4, [1, 5, 9, 13, 17], [1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4]), 4, [1, 5], [1, 2, 3, 4])),)"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Finch.execute(@finch_program_instance @loop reverse(indices)... t[indices...] = $ex2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.3",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
